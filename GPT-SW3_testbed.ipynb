{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-SW3\n",
    "https://nlu-lab.notion.site/Prompthandboken-7cd943cc230642f2b8243807731e81ae\n",
    "  \n",
    "Fick inbjudan genom att anm√§la mig via denna l√§nk: https://docs.google.com/forms/d/e/1FAIpQLSebZv__Me6YUO_kFetDZevwXgSsYSVnXWzG5H68MIwN4XmdtQ/viewform\n",
    "\n",
    "Borde inte ta l√•ngt innan man f√•r mejl, men h√∂r av om det tar f√∂r l√•ng tid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login  \n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Model name: AI-Sweden-Models/gpt-sw3-135m, AI-Sweden-Models/gpt-sw3-1.3b, AI-Sweden-Models/gpt-sw3-2.7b etc. (ju mer parametrar desto mer RAM & GPU-minne kr√§vs)\n",
    "model_name = \"AI-Sweden-Models/gpt-sw3-1.3b\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" #in case i run this on my laptop or pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AI-Sweden-Models/gpt-sw3-1.3b were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at AI-Sweden-Models/gpt-sw3-1.3b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize Tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)\n",
    "# model.eval()\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset skolmat (C:/Users/Dasmit/.cache/huggingface/datasets/amcoff___skolmat/default/0.0.0/e974832966434c6f7909c4befc52b70faa93e4095e09e728bfccebcde37bf15f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bd22d3870b4e5d883b8e2cbac15127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['meal', 'label'],\n",
      "        num_rows: 901\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = load_dataset(\"amcoff/skolmat\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['meal', 'label'],\n",
      "        num_rows: 901\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462be46862694791a2234ce0c64c6c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/901 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"meal\"], padding=\"max_length\", truncation=True)\n",
    "print(dataset)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load_metric(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: meal. If meal are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Dasmit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 901\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 226\n",
      "  Number of trainable parameters = 1343873024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7efa7a4cc34bf796b18e2c245a1914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 1 (got 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Dasmit\\Documents\\GitHub\\virtual-guide\\GPT-SW3_EXAMPLE.ipynb Cell 8\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dasmit/Documents/GitHub/virtual-guide/GPT-SW3_EXAMPLE.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# initialize Trainer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dasmit/Documents/GitHub/virtual-guide/GPT-SW3_EXAMPLE.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dasmit/Documents/GitHub/virtual-guide/GPT-SW3_EXAMPLE.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,                         \u001b[39m# the instantiated ü§ó Transformers model to be trained\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dasmit/Documents/GitHub/virtual-guide/GPT-SW3_EXAMPLE.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,                  \u001b[39m# training arguments, defined above\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dasmit/Documents/GitHub/virtual-guide/GPT-SW3_EXAMPLE.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dasmit/Documents/GitHub/virtual-guide/GPT-SW3_EXAMPLE.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Dasmit/Documents/GitHub/virtual-guide/GPT-SW3_EXAMPLE.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1540\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1542\u001b[0m )\n\u001b[1;32m-> 1543\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1544\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1545\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1546\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1547\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1548\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1765\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1762\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[0;32m   1764\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1765\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1766\u001b[0m \n\u001b[0;32m   1767\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[0;32m   1768\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1769\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\data\\data_collator.py:70\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[0;32m     71\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\data\\data_collator.py:136\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    134\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mstack([f[k] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features]))\n\u001b[0;32m    135\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([f[k] \u001b[39mfor\u001b[39;49;00m f \u001b[39min\u001b[39;49;00m features])\n\u001b[0;32m    138\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 17)"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(output_dir=\"./results\", evaluation_strategy=\"epoch\"\n",
    "                                  ,num_train_epochs=1,\n",
    "                                  per_device_train_batch_size=4,\n",
    "                                  per_device_eval_batch_size=4\n",
    "                                  )\n",
    "\n",
    "# initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,         # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F√∂ljande √§r en konversation mellan en bes√∂kare p√• museet och en guide. Guiden arbetar p√• museet. Guiden √§r hj√§lpsam, informativ och mycket v√§nlig. \n",
      "Museet inneh√•ller tre utst√§llningar, den f√∂rsta utst√§llningen heter 'Hitta nemo igen' av Hermann Gustafsson \n",
      "\n",
      "\n",
      "Bes√∂kare: Hej, jag √§r h√§r f√∂r att se utst√§llningen 'Hitta nemo igen', och jag har en fr√•ga!\n",
      "Guide: Hej, vad kul! Vad vill du veta om utst√§llningen?\n",
      "Bes√∂kare: Vet du vem som skapade utst√§llningen?\n",
      "Guide: Ja, det √§r Hermann Gustafsson, hans son.\n",
      "Bes√∂kare: √Ñr ni sl√§kt?\n",
      "Guide: Nej, det √§r inte vi.\n",
      "Bes√∂kare: Ok, jag f√∂rst√•r.\n",
      "\n",
      "\n",
      "Bes√∂kare: Hej, jag √§r intresserad av din familj. Hur m√•nga barn har ni?\n",
      "Guide: Hej! Jag har tre barn, tv√• flickor och en pojke.\n",
      "Bes√∂kare: Ok, √§r dina barn h√§r?\n",
      "Guide: Ja, de bor hos sin mamma.\n",
      "Bes√∂kare: Ok, ska vi g√• till det h√§r rummet?\n",
      "Guide: Ja, det tycker jag.\n",
      "Bes√∂kare: Ok, jag har en fr√•ga till!\n",
      "Guide: Ok, vad vill du veta om utst√§llningen?\n",
      "Bes√∂kare: Vem uppfann den?\n",
      "Guide: Hermann Gustafsson, hans son.\n",
      "Bes√∂kare: Ok, tack f√∂r hj√§lpen!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        inputs = input_ids,\n",
    "        max_new_tokens = 200,\n",
    "        do_sample=True,\n",
    "        top_p=1, temperature=0.7\n",
    "        )\n",
    "    return tokenizer.decode(output[0])\n",
    "\n",
    "prompt = \"F√∂ljande √§r en konversation mellan en bes√∂kare p√• museet och en guide. Guiden arbetar p√• museet. Guiden √§r hj√§lpsam, informativ och mycket v√§nlig. \\n\" \\\n",
    "            \"Museet inneh√•ller tre utst√§llningar, den f√∂rsta utst√§llningen heter 'Hitta nemo igen' av Hermann Gustafsson \\n\\n\\n\"\\\n",
    "            \"Bes√∂kare: Hej, jag √§r h√§r f√∂r att se utst√§llningen 'Hitta nemo igen', och jag har en fr√•ga!\\n\" \\\n",
    "            \"Guide: Hej, vad kul! Vad vill du veta om utst√§llningen?\\n\" \\\n",
    "            \"Bes√∂kare: Vet du vem som skapade utst√§llningen?\\n\" \n",
    "print(generate_text(prompt))         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
